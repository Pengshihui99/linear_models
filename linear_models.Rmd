---
title: "linear models"
author: "Shihui Peng"
date: "2023-11-24"
output: github_document
---

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
library(p8105.datasets)

set.seed(1)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## load and clean the Airbnb data

```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb |> 
  mutate(stars = review_scores_location / 2) |> 
  select(price, stars, borough = neighbourhood_group, neighbourhood, room_type) |> 
    filter(borough != "Staten Island")
```

# let's fit a model

## fit a model
```{r}
fit = lm(price ~ stars + borough, data = nyc_airbnb)

# or we can use this:
fit =
  nyc_airbnb |> 
  lm(price ~ stars + borough, data = _)

# let's look at the 'fit'
fit
```
* no need to create dummy variables by ourselves, r will recognize the categorical variables and create for us.
* The `lm` function begins with the formula specification – outcome on the left of the ~ and predictors separated by + on the right. 
  * interactions between variables can be specified using `*`.
  * You can also specify an intercept-only model (`outcome ~ 1`)
  * a model with no intercept (`outcome ~ 0 + ...`) (default is with intercept)
  * a model using all available predictors (`outcome ~ .`).

## we can use these common functions, but usually we don't
```{r}
summary(fit)
summary(fit)$coef
coef(fit)
# fitted.values(fit) --> pull out the fitted values
```
* The reason that we omit the output
  * it’s a huge pain to deal with. `summary` produces an object of class `summary.lm`, which is also a list – that’s how we extracted the coefficients using `summary(fit)$coef`. `coef` produces a vector of coefficient values, and `fitted.values` is a vector of fitted values. None of this is tidy.

## instead, we want tidy up the output (`broom::glance()`) and tidy up the coefficients (`broom::tidy()`):
```{r}
fit |> 
  broom::glance()

fit |> 
  broom::tidy()

# broom::tidy() outputs a table but this outputs a matrix (found based on...)
summary(fit)$coef |> class()
```
* `broom::tidy()` outputs a table vs `summary(fit)$coef` outputs a matrix. so broom::tidy() is better b/c a table is easier to be used.

### why we say a table is better... e.g.:
```{r}
fit |> 
  broom::tidy() |> 
  mutate(
    term = str_replace(term, "^borough", "Borough: ")
  ) |> 
  select(term, estimate, p.value) |> 
  knitr::kable(digits = 3)



```
* a good thing is everything after b::t is familiar tidyverse codes.
* As an aside, broom::tidy works with lots of things, including most of the functions for model fitting you’re likely to run into (survival, mixed models, additive models, …).

### when r creating indicator variables for categorical vars:

```{r}
fit_1 = 
  nyc_airbnb |> 
  mutate(
    borough = fct_infreq(borough),
    room_type = fct_infreq(room_type)
  ) |> 
  lm(price ~ stars + borough, data = _)

fit_1 |> 
  broom::tidy()
```
* it takes whatever cat var I has started with (e.g. stars before borough here)
* it assumes the same factor order as ggplot does (which is, alphabetical order). but we can mutate it if we need. as below:

### let's fit another model

```{r}
fit = lm(price ~ stars + borough + room_type, data = nyc_airbnb)

fit |> 
  broom::tidy()
```


# general model diagnotics with `mdoelr` package

Regression diagnostics can identify issues in model fit, especially related to certain failures in model assumptions. Examining residuals and fitted values are therefore an imporant component of any modeling exercise.

## draw some plots related to residuals
```{r}
nyc_airbnb |> 
  modelr::add_residuals(fit) |> 
  ggplot(aes(x = resid)) +
  geom_density()

# qq plot
nyc_airbnb |> 
  modelr::add_residuals(fit) |> 
  ggplot(aes(sample = resid)) +
  stat_qq() +
  stat_qq_line()

nyc_airbnb |> 
  modelr::add_residuals(fit) |> 
  ggplot(aes(x = borough, y = resid)) +
  geom_violin()

nyc_airbnb |> 
  modelr::add_residuals(fit) |> 
  ggplot(aes(x = stars, y = resid)) +
  geom_point()
```
* This example has some obvious issues, most notably the presence of extremely large outliers in price and a generally skewed residual distribution.
* There are a few things we might try to do here – including:
  * creating a formal rule for the exclusion of outliers
  * transforming the price variable (e.g. using a log transformation)
  * fitting a model that is robust to outliers. 
    * (For what it’s worth, I’d probably use a combination of median regression, which is less sensitive to outliers than OLS, and maybe bootstrapping for inference.  If that’s not feasible, I’d omit rentals with price over $1000 (< 0.5% of the sample) from the primary analysis and examine these separately.  I usually avoid transforming the outcome, because the results model is difficult to interpret.)
  
### get prediction values 

the modelr package can also provide us prediction values
```{r}
modelr::add_predictions(nyc_airbnb, fit)
```


# hypothesis test for categorical predictor

fit a 'null' and 'alternative' model - doing a test for nested models
```{r}
fit_null = lm(price ~ stars + borough, data = nyc_airbnb)
fit_alt = lm(price ~ stars + borough + room_type, data = nyc_airbnb)

anova(fit_null, fit_alt) |> 
  broom::tidy()
```
this is kinda a partial F test.

# borough-level differences

if we try to understand what effect each of these variables has and does it differ by borough...

## if we put directly add interaction terms..
```{r}
fit =
  nyc_airbnb |> 
  lm(price ~ stars*borough + room_type*borough, data = _)

fit |> 
  broom::tidy()
```
* no need to do 'stars + borough + stars x borough' stuff like what we did in SAS. just do 'stars x borough' and this includes main effects and interaction term.
* This works, but the output takes time to think through (output too many rows) – the expected change in price comparing an entire apartment to a private room in Queens, for example, involves the main effect of room type and the Queens / private room interaction.

one thing we can do is..

## fit a separate linear model for each borough

**remember, this provides same info as the above, but just easy to read.**

### using map()
```{r}
# we can do filter, based on our previous knowledge..
## nyc_airbnb |> filter(...)

# we can use this:
nyc_airbnb |> 
  nest(df = -borough) |> # nest everything into df, except for borough
  mutate(
    models = map(df, ~ lm(price ~ stars + room_type, data = .))
  )

# we can also create  a function first and then do the map():
airbnb_lm = function(df) {
  lm(price ~ stars + room_type, data = df)
}

nyc_airbnb |> 
  nest(df = -borough) |> 
  mutate(
    models = map(df, airbnb_lm),
    results = map(models, broom::tidy)
  ) |> 
  select(borough, results) |> 
  unnest(results) |> 
  select(borough, term, estimate) |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |> 
  knitr::kable(digits = 3)
```
* and then we can have a look at the slopes for variable stars for each linear reg model.
* `nest()`: The nest() function is used to create a nested data frame by grouping data based on certain variables.
* `df = -borough`: This part specifies that the nesting should be done based on the unique values of the borough variable. The `-borough` part indicates that you want to exclude the borough variable from the nested data frames and include all other variables in the data frames.

same thing but just a little different...
### use a anonymous function inside map()
```{r}
nyc_airbnb |> 
  nest(data = -borough) |> 
  mutate(
    models = map(data, \(df) lm(price ~ stars + room_type, data = df)),
    results = map(models, broom::tidy)
  ) |> 
  select(borough, results) |> 
  unnest(results) |> 
  select(borough, term, estimate) |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |> 
  knitr::kable(digits = 3)
```
* `\(df) lm(price~stars+borough, data = df)` can create a anonymous function, but this function is not saved or exists in environment. only in this line.
  * don't do this in homework

## an extreme example

Fitting models to nested datasets is a way of performing stratified analyses. These have a tradeoff: stratified models make it easy to interpret covariate effects in each stratum, but don’t provide a mechanism for assessing the significance of differences across strata.

An even more extreme example is the assessment of neighborhood effects in Manhattan. The code chunk below fits neighborhood-specific models:

```{r}
manhattan_airbnb =
  nyc_airbnb |> 
  filter(borough == "Manhattan")

manhattan_nest_lm_res =
  manhattan_airbnb |> 
  nest(data = -neighbourhood) |> 
  mutate(
    models = map(data, \(df) lm(price ~ stars + room_type, data = df)),
    results = map(models, broom::tidy)) |> 
  select(-data, -models) |> 
  unnest(results)

manhattan_nest_lm_res |> 
  filter(str_detect(term, "room_type")) |> 
  ggplot(aes(x = neighbourhood, y = estimate)) + 
  geom_point() + 
  facet_wrap(~term) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
```
* There is, generally speaking, a reduction in room price for a private room or a shared room compared to an entire apartment, but this varies quite a bit across neighborhoods.

* With this many factor levels, it really isn’t a good idea to fit models with main effects or interactions for each. Instead, you’d be best-off using a mixed model, with random intercepts and slopes for each neighborhood.  Although it’s well beyond the scope of this class, code to fit a mixed model with neighborhood-level random intercepts and random slopes for room type is below.  And, of course, we can tidy the results using a mixed-model spinoff of the broom package.


# binary outcomes

use data for homicides in Baltimore

```{r}
baltimore_df =
  read_csv("data/homicide-data.csv") |> 
  filter(city == "Baltimore") |> 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest"), # convert true and false to 1s and 0s
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")) |> 
  select(resolved, victim_age, victim_race, victim_sex)
```

## fit a logistic regression model

```{r}
fit_logistic =
  baltimore_df |> 
  glm(resolved ~ victim_age + victim_race + victim_sex,
      data = _,
      family = binomial())
```
* since this is not a continuous distribution, we need to use glm() and specify `family = binomial`

look at model results...

```{r}
fit_logistic |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate)
  ) |> 
  select(term, estimate, OR, p.value) |> 
  knitr::kable(digits = 3)
```
* bsaed on the output, we can find that:
  * Homicides in which the victim is Black are substantially less likely to be resolved that those in which the victim is white;  for other races the effects are not significant, possible due to small sample sizes.  Homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female.  The effect of age is statistically significant, but careful data inspections should be conducted before interpreting too deeply.

We can also compute fitted values; similarly to the estimates in the model summary, these are expressed as log odds and can be transformed to produce probabilities for each subject.
```{r}
baltimore_df |> 
  modelr::add_predictions(fit_logistic) |> 
  mutate(fitted_prob = boot::inv.logit(pred))
```

