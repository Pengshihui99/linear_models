---
title: "bootstrapping"
author: "Shihui Peng"
date: "2023-12-01"
output: github_document
---
```{r, message=FALSE, echo=FALSE}
library(tidyverse)
library(p8105.datasets)
library(modelr)
set.seed(1)
```

# generate a relevant example

```{r}
n_samp = 250

# create a ds w constant variance
sim_df_const =
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

# create a ds w non-constant variance
sim_df_nonconst =
  sim_df_const |> 
  mutate(
    error = error * 0.75 * x,
    y = 2 + 3 * x + error
  )

sim_df_nonconst |> 
  ggplot(aes(x=x,y=y))+geom_point(alpha=.5) + stat_smooth(method="lm")

sim_df = 
  bind_rows(const = sim_df_const, nonconst = sim_df_nonconst, .id = "data_source")

sim_df |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm") +
  facet_grid(~data_source) 
```

# fit some linear models

```{r}
sim_df_const |> 
  lm(y~x, data = _) |> 
  broom::tidy()

sim_df_nonconst |> 
  lm(y~x, data = _) |> 
  broom::tidy()
```
* but we know that non-constant error ds does not meet linear reg assumptions, so we cannot trust the CI coming out from a linear reg model.

# draw and analyze a bootstrap sample

start w a little function
```{r}
boot_sample = function(df){
  sample_frac(df, replace = TRUE)
}
```
* `sample_frac()` know how to sample n rows from a table. the defaulted `sample_frac()` is sampling size 100% as large as the df
  * `replace = TRUE`: each sampled element is put back into the population before the next element is selected -- the same element can be selected more than once in the sample.
  * if `replace = FALSE`: each sampled element is unique, meaning once an element is selected, it is not available for selection again. The next sampling is done from the remaining elements. This is the default behavior for sampling functions in R. --- we do not want this here.

let's see how it works!
```{r}
sim_df_nonconst |> 
  boot_sample() |> 
  ggplot(aes(x=x, y=y)) + geom_point(alpha = 0.5) + stat_smooth(method = "lm")
```
* from the scatterplot, we can see some points are darker and some are lighter. the darker ones means they are picked many times through the `boot_sample function` we just defined

# draw a lot of samples and analyze them 

```{r}
boot_straps = 
  tibble(strap_number = 1:1000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(sim_df_nonconst))
  )

# check the 1st tibble of the output
boot_straps |> 
  pull(strap_sample) |> 
  nth(1) |> 
  arrange(x)
```
* `strap_sample = map(strap_number, \(i) boot_sample(sim_df_nonconst))`: `boot_sample()` does not really depend on `i`, this is just saying that for each iteration and strap_number, run exactly this line of code.

now do the `lm` fit
```{r}
boot_results = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(y~x, data = df)),
    results = map(models, broom::tidy)
  ) |> 
  select(strap_number, results) |> 
  unnest(results)
```

try to summarise these results -- get a bootstrap SE

```{r}
boot_results |> 
  group_by(term) |> 
  summarise(
    se = sd(estimate)
  ) |> 
  knitr::kable()
```

look at the distribution

```{r}
boot_results |> 
  ggplot(aes(x = estimate)) + geom_density() + facet_grid(.~term)
```

can i construct a CI?

```{r}
boot_results |> 
  group_by(term) |> 
  summarise(
    ci_lower = quantile(estimate, 0.025),
    ci_upper = quantile(estimate, 0.975)
  ) 
```
* if you are in a case where you have the parameter estimate distribution doesn't follow a normal distribution, and it is kind of skewed, then you might need more bootstrap samples to construct that CI -- the # of bootstrap does not really depend on sample size
